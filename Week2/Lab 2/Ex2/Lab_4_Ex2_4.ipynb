{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sin(5 * x[1] - 5) * np.exp(((1 - np.cos(5 * x[0] - 5)) ** 2)) + np.cos(5 * x[0] - 5) * np.exp(((1 - np.sin(5 * x[1] - 5)) ** 2)) + 25 * (x[0] - x[1]) ** 2   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x):\n",
    "    grad = np.zeros((2,1))\n",
    "\n",
    "    grad[0][0] = 5 * np.cos(5 * x[0] - 5) * np.exp(((1 - np.cos(5 * x[0] - 5)) ** 2)) - 25 * (x[0] - x[1])        \n",
    "    grad[1][0] = -5 * np.sin(5 * x[1] - 5) * np.exp(((1 - np.sin(5 * x[1] - 5)) ** 2)) +  25 * (x[0] - x[1])\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_size(R, norm_grad_matrix):\n",
    "    return R/np.linalg.norm(norm_grad_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X before iteration =  [[0]\n",
      " [0]]\n",
      "Norm_grad_matrix =  [[5.35535386]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_33076\\3421823251.py:4: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  grad[0][0] = 5 * np.cos(5 * x[0] - 5) * np.exp(((1 - np.cos(5 * x[0] - 5)) ** 2)) - 25 * (x[0] - x[1])\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_33076\\3421823251.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  grad[1][0] = -5 * np.sin(5 * x[1] - 5) * np.exp(((1 - np.sin(5 * x[1] - 5)) ** 2)) +  25 * (x[0] - x[1])\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_33076\\2118741085.py:21: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  diff = (math.sqrt((x[0]-i/100)**2 + (x[1] - j/100)**2)) ** 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  0.9998499887483121\n",
      "W =  [[-0.44235601]\n",
      " [ 0.89667227]]\n",
      "Z =  [[-0.44]\n",
      " [ 0.89]]\n",
      "X updated =  [[-0.22 ]\n",
      " [ 0.445]]\n",
      "\n",
      "X before iteration =  [[-0.22 ]\n",
      " [ 0.445]]\n",
      "Norm_grad_matrix =  [[ 5.35535386]\n",
      " [22.18031263]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_33076\\2118741085.py:45: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  diff = (math.sqrt((w[0]-i/100)**2 + (w[1] - j/100)**2)) ** 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  1.2478481478128658\n",
      "W =  [[-1.39812385]\n",
      " [ 0.73374712]]\n",
      "Z =  [[-0.88]\n",
      " [ 0.47]]\n",
      "X updated =  [[-0.44 ]\n",
      " [ 0.235]]\n",
      "\n",
      "X before iteration =  [[-0.44 ]\n",
      " [ 0.235]]\n",
      "Norm_grad_matrix =  [[ 5.35535386]\n",
      " [22.18031263]\n",
      " [28.929595  ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  1.4331521203277757\n",
      "W =  [[-1.23430784]\n",
      " [ 1.03205156]]\n",
      "Z =  [[-0.75]\n",
      " [ 0.66]]\n",
      "X updated =  [[-0.375]\n",
      " [ 0.33 ]]\n",
      "\n",
      "X before iteration =  [[-0.375]\n",
      " [ 0.33 ]]\n",
      "Norm_grad_matrix =  [[ 5.35535386]\n",
      " [22.18031263]\n",
      " [28.929595  ]\n",
      " [29.36428449]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  1.378160005224357\n",
      "W =  [[-1.01549113]\n",
      " [ 0.90230861]]\n",
      "Z =  [[-0.75]\n",
      " [ 0.66]]\n",
      "X updated =  [[-0.375]\n",
      " [ 0.33 ]]\n",
      "\n",
      "X before iteration =  [[-0.375]\n",
      " [ 0.33 ]]\n",
      "Norm_grad_matrix =  [[ 5.35535386]\n",
      " [22.18031263]\n",
      " [28.929595  ]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  1.378160005224357\n",
      "W =  [[-0.9185634 ]\n",
      " [ 0.81569917]]\n",
      "Z =  [[-0.75]\n",
      " [ 0.66]]\n",
      "X updated =  [[-0.375]\n",
      " [ 0.33 ]]\n",
      "\n",
      "X before iteration =  [[-0.375]\n",
      " [ 0.33 ]]\n",
      "Norm_grad_matrix =  [[ 5.35535386]\n",
      " [22.18031263]\n",
      " [28.929595  ]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  1.378160005224357\n",
      "W =  [[-0.8554908 ]\n",
      " [ 0.75934087]]\n",
      "Z =  [[-0.75]\n",
      " [ 0.66]]\n",
      "X updated =  [[-0.375]\n",
      " [ 0.33 ]]\n",
      "\n",
      "X before iteration =  [[-0.375]\n",
      " [ 0.33 ]]\n",
      "Norm_grad_matrix =  [[ 5.35535386]\n",
      " [22.18031263]\n",
      " [28.929595  ]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  1.378160005224357\n",
      "W =  [[-0.81026456]\n",
      " [ 0.71892912]]\n",
      "Z =  [[-0.75]\n",
      " [ 0.66]]\n",
      "X updated =  [[-0.375]\n",
      " [ 0.33 ]]\n",
      "\n",
      "X before iteration =  [[-0.375]\n",
      " [ 0.33 ]]\n",
      "Norm_grad_matrix =  [[ 5.35535386]\n",
      " [22.18031263]\n",
      " [28.929595  ]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  1.378160005224357\n",
      "W =  [[-0.77579701]\n",
      " [ 0.68813076]]\n",
      "Z =  [[-0.75]\n",
      " [ 0.66]]\n",
      "X updated =  [[-0.375]\n",
      " [ 0.33 ]]\n",
      "\n",
      "X before iteration =  [[-0.375]\n",
      " [ 0.33 ]]\n",
      "Norm_grad_matrix =  [[ 5.35535386]\n",
      " [22.18031263]\n",
      " [28.929595  ]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [ 0.        ]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  1.378160005224357\n",
      "W =  [[-0.74840363]\n",
      " [ 0.66365351]]\n",
      "Z =  [[-0.75]\n",
      " [ 0.66]]\n",
      "X updated =  [[-0.375]\n",
      " [ 0.33 ]]\n",
      "\n",
      "X before iteration =  [[-0.375]\n",
      " [ 0.33 ]]\n",
      "Norm_grad_matrix =  [[ 5.35535386]\n",
      " [22.18031263]\n",
      " [28.929595  ]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [29.36428449]\n",
      " [29.36428449]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  1.378160005224357\n",
      "W =  [[-0.72595455]\n",
      " [ 0.64359421]]\n",
      "Z =  [[-0.73]\n",
      " [ 0.64]]\n",
      "X updated =  [[-0.365]\n",
      " [ 0.32 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ada_grad(x0, max_iter_T):\n",
    "    x = x0\n",
    "    t = 0\n",
    "    T = max_iter_T\n",
    "\n",
    "    norm_grad_matrix = np.zeros((T,1))\n",
    "\n",
    "    while (t<T):\n",
    "\n",
    "      print(\"X before iteration = \", x)\n",
    "\n",
    "      norm_grad_matrix[t] = np.linalg.norm(grad(x))\n",
    "\n",
    "      print(\"Norm_grad_matrix = \", norm_grad_matrix)\n",
    "\n",
    "      max = 0\n",
    "      y = np.zeros((2,1))\n",
    "      for i in range(-100,100):\n",
    "        for j in range(-100,100):\n",
    "           diff = 0\n",
    "           diff = (math.sqrt((x[0]-i/100)**2 + (x[1] - j/100)**2)) ** 2\n",
    "           if (diff>max and math.sqrt((i/100)**2 + (j/100)**2) <= 1): \n",
    "                  min = diff\n",
    "                  y[0] = i/100\n",
    "                  y[1] = j/100\n",
    "      print(\"Y = \",y)\n",
    "\n",
    "      R = np.linalg.norm(x-y)\n",
    "      print(\"R = \", R)\n",
    "\n",
    "      a = step_size(R, norm_grad_matrix)\n",
    "\n",
    "     \n",
    "\n",
    "      w = x - a*grad(x)\n",
    "\n",
    "      print(\"W = \", w)\n",
    "\n",
    "      z = np.zeros((2,1))\n",
    "\n",
    "      min = 10000000\n",
    "      for i in range(-100,100):\n",
    "              for j in range(-100,100):\n",
    "                  diff = 0\n",
    "                  diff = (math.sqrt((w[0]-i/100)**2 + (w[1] - j/100)**2)) ** 2\n",
    "                  if (diff<min and math.sqrt((i/100)**2 + (j/100)**2) <= 1): \n",
    "                      min = diff\n",
    "                      z[0] = i/100\n",
    "                      z[1] = j/100\n",
    "\n",
    "      print(\"Z = \", z)\n",
    "\n",
    "      x = 0.5 * z\n",
    "\n",
    "      t = t+1\n",
    "\n",
    "      print(\"X updated = \", x)\n",
    "      print()\n",
    "\n",
    "\n",
    "    return x\n",
    "\n",
    "x0 = np.array([[0],[0]])\n",
    "\n",
    "x_min = ada_grad(x0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X before iteration =  [[0]\n",
      " [0]]\n",
      "Norm_grad_matrix =  [[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_33076\\759566366.py:8: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  grad[0][0] = -400*x[0]*(x[1] - x[0]**2) - 2*(0.5 - x[0])\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_33076\\759566366.py:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  grad[1][0] = 200*(x[1] - x[0]**2)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_33076\\759566366.py:34: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  diff = (math.sqrt((x[0]-i/100)**2 + (x[1] - j/100)**2)) ** 2\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_33076\\759566366.py:58: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  diff = (math.sqrt((w[0]-i/100)**2 + (w[1] - j/100)**2)) ** 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  0.9998499887483121\n",
      "W =  [[0.99984999]\n",
      " [0.        ]]\n",
      "Z =  [[0.99]\n",
      " [0.  ]]\n",
      "X updated =  [[0.495]\n",
      " [0.   ]]\n",
      "\n",
      "X before iteration =  [[0.495]\n",
      " [0.   ]]\n",
      "Norm_grad_matrix =  [[ 1.        ]\n",
      " [68.95085351]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  0.5144171459039832\n",
      "W =  [[0.13316033]\n",
      " [0.36556997]]\n",
      "Z =  [[0.13]\n",
      " [0.37]]\n",
      "X updated =  [[0.065]\n",
      " [0.185]]\n",
      "\n",
      "X before iteration =  [[0.065]\n",
      " [0.185]]\n",
      "Norm_grad_matrix =  [[ 1.        ]\n",
      " [68.95085351]\n",
      " [36.58156087]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  0.9260939477180488\n",
      "W =  [[ 0.13108319]\n",
      " [-0.24393597]]\n",
      "Z =  [[ 0.13]\n",
      " [-0.24]]\n",
      "X updated =  [[ 0.065]\n",
      " [-0.12 ]]\n",
      "\n",
      "X before iteration =  [[ 0.065]\n",
      " [-0.12 ]]\n",
      "Norm_grad_matrix =  [[ 1.        ]\n",
      " [68.95085351]\n",
      " [36.58156087]\n",
      " [24.95682105]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  0.9608459814142952\n",
      "W =  [[0.03733224]\n",
      " [0.17129202]]\n",
      "Z =  [[0.04]\n",
      " [0.17]]\n",
      "X updated =  [[0.02 ]\n",
      " [0.085]]\n",
      "\n",
      "X before iteration =  [[0.02 ]\n",
      " [0.085]]\n",
      "Norm_grad_matrix =  [[ 1.        ]\n",
      " [68.95085351]\n",
      " [36.58156087]\n",
      " [24.95682105]\n",
      " [16.99898568]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  0.9715580270884493\n",
      "W =  [[ 0.03899997]\n",
      " [-0.1114073 ]]\n",
      "Z =  [[ 0.04]\n",
      " [-0.11]]\n",
      "X updated =  [[ 0.02 ]\n",
      " [-0.055]]\n",
      "\n",
      "X before iteration =  [[ 0.02 ]\n",
      " [-0.055]]\n",
      "Norm_grad_matrix =  [[ 1.        ]\n",
      " [68.95085351]\n",
      " [36.58156087]\n",
      " [24.95682105]\n",
      " [16.99898568]\n",
      " [11.0920459 ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  0.9894063876891032\n",
      "W =  [[0.02605627]\n",
      " [0.07484413]]\n",
      "Z =  [[0.03]\n",
      " [0.07]]\n",
      "X updated =  [[0.015]\n",
      " [0.035]]\n",
      "\n",
      "X before iteration =  [[0.015]\n",
      " [0.035]]\n",
      "Norm_grad_matrix =  [[ 1.        ]\n",
      " [68.95085351]\n",
      " [36.58156087]\n",
      " [24.95682105]\n",
      " [16.99898568]\n",
      " [11.0920459 ]\n",
      " [ 7.05416479]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  0.9806375477208691\n",
      "W =  [[ 0.0286424]\n",
      " [-0.0455013]]\n",
      "Z =  [[ 0.03]\n",
      " [-0.05]]\n",
      "X updated =  [[ 0.015]\n",
      " [-0.025]]\n",
      "\n",
      "X before iteration =  [[ 0.015]\n",
      " [-0.025]]\n",
      "Norm_grad_matrix =  [[ 1.        ]\n",
      " [68.95085351]\n",
      " [36.58156087]\n",
      " [24.95682105]\n",
      " [16.99898568]\n",
      " [11.0920459 ]\n",
      " [ 7.05416479]\n",
      " [ 5.11098942]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  0.9888629834309706\n",
      "W =  [[0.02453768]\n",
      " [0.03377677]]\n",
      "Z =  [[0.02]\n",
      " [0.03]]\n",
      "X updated =  [[0.01 ]\n",
      " [0.015]]\n",
      "\n",
      "X before iteration =  [[0.01 ]\n",
      " [0.015]]\n",
      "Norm_grad_matrix =  [[ 1.        ]\n",
      " [68.95085351]\n",
      " [36.58156087]\n",
      " [24.95682105]\n",
      " [16.99898568]\n",
      " [11.0920459 ]\n",
      " [ 7.05416479]\n",
      " [ 5.11098942]\n",
      " [ 3.15613184]\n",
      " [ 0.        ]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  0.9879397754924133\n",
      "W =  [[ 0.02209219]\n",
      " [-0.01966212]]\n",
      "Z =  [[ 0.02]\n",
      " [-0.02]]\n",
      "X updated =  [[ 0.01]\n",
      " [-0.01]]\n",
      "\n",
      "X before iteration =  [[ 0.01]\n",
      " [-0.01]]\n",
      "Norm_grad_matrix =  [[ 1.        ]\n",
      " [68.95085351]\n",
      " [36.58156087]\n",
      " [24.95682105]\n",
      " [16.99898568]\n",
      " [11.0920459 ]\n",
      " [ 7.05416479]\n",
      " [ 5.11098942]\n",
      " [ 3.15613184]\n",
      " [ 2.22783486]]\n",
      "Y =  [[0.99]\n",
      " [0.14]]\n",
      "R =  0.9914131328563284\n",
      "W =  [[0.02096369]\n",
      " [0.0135703 ]]\n",
      "Z =  [[0.02]\n",
      " [0.01]]\n",
      "X updated =  [[0.01 ]\n",
      " [0.005]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    f = 100*(x[1]-x[0]**2)**2 + (0.5-x[0])**2\n",
    "    return f\n",
    "\n",
    "def grad(x):\n",
    "    grad = np.zeros((2,1))\n",
    "\n",
    "    grad[0][0] = -400*x[0]*(x[1] - x[0]**2) - 2*(0.5 - x[0])\n",
    "    grad[1][0] = 200*(x[1] - x[0]**2)\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def ada_grad(x0, max_iter_T):\n",
    "    x = x0\n",
    "    t = 0\n",
    "    T = max_iter_T\n",
    "\n",
    "    norm_grad_matrix = np.zeros((T,1))\n",
    "\n",
    "    while (t<T):\n",
    "\n",
    "      print(\"X before iteration = \", x)\n",
    "\n",
    "      norm_grad_matrix[t] = np.linalg.norm(grad(x))\n",
    "\n",
    "      print(\"Norm_grad_matrix = \", norm_grad_matrix)\n",
    "\n",
    "      max = 0\n",
    "      y = np.zeros((2,1))\n",
    "      for i in range(-100,100):\n",
    "        for j in range(-100,100):\n",
    "           diff = 0\n",
    "           diff = (math.sqrt((x[0]-i/100)**2 + (x[1] - j/100)**2)) ** 2\n",
    "           if (diff>max and math.sqrt((i/100)**2 + (j/100)**2) <= 1): \n",
    "                  min = diff\n",
    "                  y[0] = i/100\n",
    "                  y[1] = j/100\n",
    "      print(\"Y = \",y)\n",
    "\n",
    "      R = np.linalg.norm(x-y)\n",
    "      print(\"R = \", R)\n",
    "\n",
    "      a = step_size(R, norm_grad_matrix)\n",
    "      \n",
    "      w = x - a*grad(x)\n",
    "\n",
    "      print(\"W = \", w)\n",
    "\n",
    "      z = np.zeros((2,1))\n",
    "\n",
    "      min = 10000000\n",
    "      for i in range(-100,100):\n",
    "              for j in range(-100,100):\n",
    "                  diff = 0\n",
    "                  diff = (math.sqrt((w[0]-i/100)**2 + (w[1] - j/100)**2)) ** 2\n",
    "                  if (diff<min and math.sqrt((i/100)**2 + (j/100)**2) <= 1): \n",
    "                      min = diff\n",
    "                      z[0] = i/100\n",
    "                      z[1] = j/100\n",
    "\n",
    "      print(\"Z = \", z)\n",
    "\n",
    "      x = 0.5 * z\n",
    "\n",
    "      t = t+1\n",
    "\n",
    "      print(\"X updated = \", x)\n",
    "      print()\n",
    "\n",
    "\n",
    "    return x\n",
    "\n",
    "x0 = np.array([[0],[0]])\n",
    "\n",
    "x_min = ada_grad(x0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_33076\\2819654300.py:14: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  grad[0][0] = 5 * np.cos(5 * x[0] - 5) * np.exp(((1 - np.cos(5 * x[0] - 5)) ** 2)) - \\\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_33076\\2819654300.py:16: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  grad[1][0] = -5 * np.sin(5 * x[1] - 5) * np.exp(((1 - np.sin(5 * x[1] - 5)) ** 2)) + \\\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_33076\\2819654300.py:42: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  diff = (m.sqrt((x[0]-i/100)**2 + (x[1] - j/100)**2)) ** 2\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_33076\\2819654300.py:62: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  diff = (m.sqrt((w[0]-i/100)**2 + (w[1] - j/100)**2)) ** 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max iterations =  100\n",
      "The minimiser =  [[-0.045]\n",
      " [ 0.055]]\n",
      "The minima =  [2.03681035]\n",
      "The error =  [103.68103544]\n",
      "\n",
      "For max iterations =  500\n",
      "The minimiser =  [[-0.02]\n",
      " [ 0.03]]\n",
      "The minima =  [1.89901845]\n",
      "The error =  [89.90184462]\n",
      "\n",
      "For max iterations =  1000\n",
      "The minimiser =  [[-0.015]\n",
      " [ 0.025]]\n",
      "The minima =  [1.89125098]\n",
      "The error =  [89.12509797]\n",
      "\n",
      "For max iterations =  5000\n",
      "The minimiser =  [[-0.01 ]\n",
      " [ 0.015]]\n",
      "The minima =  [1.87581294]\n",
      "The error =  [87.58129447]\n",
      "\n",
      "For max iterations =  10000\n",
      "The minimiser =  [[-0.005]\n",
      " [ 0.01 ]]\n",
      "The minima =  [1.88328687]\n",
      "The error =  [88.32868749]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math as m\n",
    "\n",
    "def func(x):\n",
    "    f = np.sin(5 * x[1] - 5) * np.exp(((1 - np.cos(5 * x[0] - 5)) ** 2)) + \\\n",
    "         np.cos(5 * x[0] - 5) * np.exp(((1 - np.sin(5 * x[1] - 5)) ** 2)) + \\\n",
    "         25 * (x[0] - x[1]) ** 2\n",
    "    return f\n",
    "\n",
    "def gradient(x):\n",
    "    grad = np.zeros((2,1))\n",
    "\n",
    "    grad[0][0] = 5 * np.cos(5 * x[0] - 5) * np.exp(((1 - np.cos(5 * x[0] - 5)) ** 2)) - \\\n",
    "         25 * (x[0] - x[1])\n",
    "    grad[1][0] = -5 * np.sin(5 * x[1] - 5) * np.exp(((1 - np.sin(5 * x[1] - 5)) ** 2)) + \\\n",
    "         25 * (x[0] - x[1])\n",
    "\n",
    "    return grad\n",
    "\n",
    "def step_size_a(R, norm_grad_matrix):\n",
    "    a = R/norm(norm_grad_matrix)\n",
    "    return a\n",
    "\n",
    "def norm(m):\n",
    "    return np.linalg.norm(m)\n",
    "\n",
    "def ada_grad(x0, max_iter_T):\n",
    "    x = x0\n",
    "    t = 0\n",
    "    T = max_iter_T\n",
    "\n",
    "    norm_grad_matrix = np.zeros((T,1))\n",
    "\n",
    "    while (t<T):\n",
    "      norm_grad_matrix[t] = norm(gradient(x))\n",
    "      max = 0\n",
    "      y = np.zeros((2,1))\n",
    "      for i in range(-100,100):\n",
    "        for j in range(-100,100):\n",
    "           diff = 0\n",
    "           diff = (m.sqrt((x[0]-i/100)**2 + (x[1] - j/100)**2)) ** 2\n",
    "           if (diff>max and m.sqrt((i/100)**2 + (j/100)**2) <= 1): \n",
    "                  min = diff\n",
    "                  y[0] = i/100\n",
    "                  y[1] = j/100\n",
    "\n",
    "      R = norm(x-y)\n",
    "    \n",
    "\n",
    "      a = step_size_a(R, norm_grad_matrix)\n",
    "\n",
    "      w = x - a*gradient(x)\n",
    "\n",
    "\n",
    "      z = np.zeros((2,1))\n",
    "\n",
    "      min = 10000000\n",
    "      for i in range(-100,100):\n",
    "              for j in range(-100,100):\n",
    "                  diff = 0\n",
    "                  diff = (m.sqrt((w[0]-i/100)**2 + (w[1] - j/100)**2)) ** 2\n",
    "                  if (diff<min and m.sqrt((i/100)**2 + (j/100)**2) <= 1):\n",
    "                      min = diff\n",
    "                      z[0] = i/100\n",
    "                      z[1] = j/100\n",
    "      x = 0.5 * z\n",
    "      t = t+1\n",
    "      per_error = (func(x) - 1) * 100 \n",
    "\n",
    "    return x, func(x), per_error\n",
    "\n",
    "\n",
    "x0 = np.array([[0],[0]])\n",
    "\n",
    "T = [10**2, 500, 10**3, 5000, 10**4, 50000, 10**5, 500000, 10**6, 5000000]\n",
    "\n",
    "t = []\n",
    "e = []\n",
    "\n",
    "for i in range(len(T)):\n",
    "\n",
    "    x_min, min, error_percent = ada_grad(x0, T[i])\n",
    "\n",
    "    print(\"For max iterations = \", T[i])\n",
    "    print(\"The minimiser = \", x_min)\n",
    "    print(\"The minima = \", min)\n",
    "    print(\"The error = \", error_percent)\n",
    "    print()\n",
    "\n",
    "    t.append(T[i])\n",
    "    e.append(error_percent)\n",
    "   \n",
    "\n",
    "plt.plot(t,e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
