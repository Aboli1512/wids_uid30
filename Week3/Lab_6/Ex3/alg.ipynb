{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 3257.946416099905 seconds\n",
      "||∇fλ(x*)||: 0.020051524507800435\n",
      "||Ax* - y||^2: 0.027141569433024818\n",
      "||x* - xorig||^2: 49804.12796042601\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "# Set random seed for repeatability\n",
    "np.random.seed(1000)\n",
    "\n",
    "# Parameters\n",
    "N = 200\n",
    "d = 50000\n",
    "  # Dimension that caused failure in the previous experiment\n",
    "lambda_reg = 0.001\n",
    "epochs = int(1e4)\n",
    "\n",
    "# Generate random noise\n",
    "eps = np.random.randn(N, 1)\n",
    "\n",
    "# Create data matrix and label vector\n",
    "A = np.random.randn(N, d)\n",
    "\n",
    "# Normalize the columns of A\n",
    "for j in range(A.shape[1]):\n",
    "    A[:, j] = A[:, j] / np.linalg.norm(A[:, j])\n",
    "\n",
    "# Original x\n",
    "xorig = np.ones((d, 1))\n",
    "\n",
    "# Generate y\n",
    "y = np.dot(A, xorig) + eps\n",
    "\n",
    "# Initialize optimization variable\n",
    "x = np.zeros((d, 1))\n",
    "\n",
    "# Start the timer\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Index array\n",
    "arr = np.arange(N)\n",
    "\n",
    "# Run the algorithm\n",
    "t = 1\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(arr)  # Shuffle every epoch\n",
    "    for i in np.nditer(arr):  # Pass through the data points\n",
    "        Ai = A[i, :].reshape(1, -1)\n",
    "        yi = y[i]\n",
    "        grad = Ai.T @ (Ai @ x - yi) + lambda_reg * x\n",
    "        x = x - (1 / t) * grad\n",
    "        t += 1\n",
    "        if t > 1e4:\n",
    "            t = 1\n",
    "\n",
    "# Stop the timer\n",
    "alglab6time = timeit.default_timer() - start\n",
    "\n",
    "# Results\n",
    "x_alglab6 = x\n",
    "norm_grad = np.linalg.norm(grad)\n",
    "norm_residual = np.linalg.norm(A @ x_alglab6 - y) ** 2\n",
    "norm_diff = np.linalg.norm(x_alglab6 - xorig) ** 2\n",
    "\n",
    "print(f\"Time taken: {alglab6time} seconds\")\n",
    "print(f\"||∇fλ(x*)||: {norm_grad}\")\n",
    "print(f\"||Ax* - y||^2: {norm_residual}\")\n",
    "print(f\"||x* - xorig||^2: {norm_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 45.49084089999087 seconds\n",
      "||∇fλ(x∗)||: 0.2848006037559653\n",
      "||Ax∗ - y||²: 11.03848539933142\n",
      "||x∗ - xorig||²: 11.687490029344573\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "np.random.seed()  # for repeatability\n",
    "\n",
    "# Parameters\n",
    "N = 200  # Number of data points\n",
    "d = 100  # Dimension of each data point (adjust as per failure dimension in previous experiment)\n",
    "lambda_reg = 0.001\n",
    "eps = np.random.randn(N, 1)  # Random noise\n",
    "\n",
    "# Create data matrix A and label vector y\n",
    "A = np.random.randn(N, d)\n",
    "\n",
    "# Normalize the columns\n",
    "for j in range(A.shape[1]):\n",
    "    A[:, j] = A[:, j] / np.linalg.norm(A[:, j])\n",
    "\n",
    "# True underlying parameter vector xorig\n",
    "xorig = np.ones((d, 1))\n",
    "\n",
    "# Label vector y = Axorig + noise\n",
    "y = np.dot(A, xorig) + eps\n",
    "\n",
    "# Initialize optimization variable x\n",
    "x = np.zeros((d, 1))\n",
    "\n",
    "# Parameters for optimization\n",
    "epochs = 1e4  # Number of epochs\n",
    "t = 1  # Initial learning rate\n",
    "\n",
    "# Index array for shuffling\n",
    "arr = np.arange(N)\n",
    "\n",
    "# Start the timer\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Stochastic gradient descent (SGD) loop\n",
    "for epoch in range(int(epochs)):\n",
    "    np.random.shuffle(arr)  # Shuffle the indices every epoch\n",
    "    for i in np.nditer(arr):  # Pass through data points\n",
    "        # Compute the gradient gi(x) for the current data point\n",
    "        # For example, assume gi(x) = A[i, :]^T * (A[i, :] * x - y[i])\n",
    "        gi = A[i, :].reshape(-1, 1) * (np.dot(A[i, :], x) - y[i])\n",
    "\n",
    "        # Update x using the gradient descent step\n",
    "        x -= 1/t * gi\n",
    "\n",
    "        # Update the learning rate (optional)\n",
    "        t += 1\n",
    "        if t > 1e4:\n",
    "            t = 1\n",
    "\n",
    "# End the timer\n",
    "alglab6time = timeit.default_timer() - start\n",
    "\n",
    "# Compute the desired results\n",
    "Ax_alglab6 = np.dot(A, x)\n",
    "grad_norm = np.linalg.norm(gi)  # ∇fλ(x∗)\n",
    "Ax_diff = np.linalg.norm(Ax_alglab6 - y)  # ||Ax∗ - y||²\n",
    "x_diff = np.linalg.norm(x - xorig)  # ||x∗ - xorig||²\n",
    "\n",
    "print(f\"Time taken: {alglab6time} seconds\")\n",
    "print(f\"||∇fλ(x∗)||: {grad_norm}\")\n",
    "print(f\"||Ax∗ - y||²: {Ax_diff}\")\n",
    "print(f\"||x∗ - xorig||²: {x_diff}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
